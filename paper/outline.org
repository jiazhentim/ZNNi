
* ZNNi - Maximizing the Inference Throughput of 3D Convolutional Networks on Multi-Core Machines and GPUs.
** Claims
*** Use large output patches

    Saturation ~ to the field of view

*** Don't use batch inputs
*** Speed depends on the network depth, width and filter sizes
*** Worth spending time on optimizing the performances as it will save more time overall
*** Instead of buying 3 extra GPUs to get 4x improvement in the throughput, consider buying extra memory (cheaper and possibly better)
*** question: how/why is this different from training
**** throughput rather than latency (often orthogonal goals)
**** no images need to be stored from forward to backward pass
** Intro

   Explain convolutional networks, how the forward pass works
   Explain how it can be done using FFTs

** Algorithms
*** Max fragment pooling

    With restricted output sizes to make sure all the batches have the
    same size

*** CPU
**** Version 1
     - Perform the sequential algorithm.
       - calculate the FFTs of all the inputs
       - for each output
         - for each incoming kernel
           - FFT of kernel
           - point-wise multiply add with the appropriate input FFT 
         - end
         - inverse FFT of the output image
       - end
     - Each FFT is parallelized by having multiple thread do a subset
       of 1D convolutions across first x, then y then z - 3 sync
       points.
     - Point-wise operations on the images are also parallelized by
       having multiple threads do different subset of elements [linear
       separation in memmory].
**** Version 2
    - (batch x number of inputs) tasks are generated for computing
      FFTs of the inputs.  Execute in parallel on n workers and wait
      until the last one completes.
    - each of n workers
      - takes kernel from least complete output (removes output from lock-free list)
      - computes kernel FFT
      - creates sub-tasks for each input in a batch for multiply-adds associated with this kernel and this output
      - adds output back to lock-free list
    - (batch x number of outputs) tasks generated for inverse FFTs of
      outputs. Execute in parallel on n workers and wait until the
      last one completes.
*** GPU (onboard RAM)
**** direct convolution
***** cuDNN primitives for 3D convolution (GEMM or precomputed GEMM)
**** FFT-based algorithm
***** loop over batch
****** compute FFTs of all inputs in layer
***** release memory used for inputs
***** loop over outputs in layer
****** compute FFTs of all incident kernels
****** loop over batch
******* point-wise multiplication of all input and kernel FFTs
******* accumulation of output expressed as matrix multiplication
***** release memory used for input FFTs
***** loop over batch
****** inverse FFT of all outputs in layer
***** release memory used for output FFTs
**** Padded Pruned FFTs
     - FFTs parallelized over batches of 1D FFTs using cuFFT
       (cufftMakePlanMany).
     - Memory reshuffled - transposed before each set of 1D FFTs -
       parallelized using cuda Thrust and fast div/mod
     - Taking N FFTs at once where N is the number of input
       feature-maps of the layer.
     - Accumulating the FFT of the output feature-map as a matrix
       vector multiply,  parallelized using by cuBLAS gemv.
*** GPU (onboard and host RAM)
    Move just a subset of the network to the GPU to perform
    computation, either direct or FFT.
*** CPU-GPU Fusion

    CPU on the top layers, generating batches (pooling networks).  A
    single batch can then fit on the GPU and is being executed there.

    Pipeline form.

    More limits on the size of the network b/c we need to keep extra
    data in memory (for the pipeline)

** Experiments

   Done on 3D networks with relatively large fields of view

*** Purely convolutional networks
*** Convolutional networks with pooling layers
** Contributions

   Parallel CPU algorithms
   Pruned FFTs on the CPU

   Parallel GPU algorithms using 1D FFTs.
