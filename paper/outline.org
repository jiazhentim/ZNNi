
* ZNNi - Maximizing the Inference Throughput of 3D Convolutional Networks on Multi-Core CPUs and GPUs
** short abstract (150 words)
Sliding window convolutional networks (ConvNets) have become a popular
approach to computer vision problems such as image segmentation, and
object detection and localization. Here we consider the
parallelization of inference, i.e., the application of a previously
trained ConvNet, with emphasis on 3D images.  Our goal is to maximize
throughput, defined as the number of output voxels computed per unit
time.  We propose CPU and GPU primitives for convolutional and pooling
layers, which are combined to create CPU, GPU, and CPU-GPU inference
algorithms.  The primitives include convolution based on highly
efficient padded and pruned FFTs. Our theoretical analyses and
empirical tests reveal a number of interesting findings.  For example,
adding host RAM can be a more efficient way of increasing throughput
than adding another GPU or more CPUs.  Furthermore, our CPU-GPU
algorithm can achieve greater throughput than the sum of CPU-only and
GPU-only throughputs.
** Abstract
  Sliding window convolutional networks (ConvNets) have become a
  popular approach to computer vision problems such as image
  segmentation, and object detection and localization. A sizable
  literature already exists on parallelizing ConvNet training on GPUs
  and CPUs.  Here we consider the parallelization of inference, i.e.,
  the application of a previously trained ConvNet, with the goal of maximizing throughput.

  Th CPU-only and GPU-only algorithms are based on a set of primitives
  for convolutional and pooling layers.  For a given ConvNet
  architecture, the best algorithm is found by choosing the primitive
  for each layer and the input size to maximize throughput, defined as
  number of output voxels computed per unit time. The maximization is
  subject to a number of constraints, chief among them a limit on RAM usage. 
  
  our algorithms are designed to minimize the
  memory overhead, allowing us to process very large inputs.  Using
  "fragment pooling" instead of regular pooling allows us to reduce
  the amount of computation.  We implement the convolutional layers
  with a novel FFT-based parallel algorithms for the convolutional
  layers that are based on highly efficient padded and pruned FFTs,
  and have minimal memory overhead.  For small filter sizes we use can
  utilize the CuDNN primitives on the GPU.  We show that both the
  multi-core CPU and the GPU algorithms greatly benefit from the
  amount of memory available, where the GPUs can also benefit from the
  amount of the host RAM.  We also propose an algorithm that uses all
  available resources -- the CPUs and the GPUs.  We show that our
  approach greatly outperforms other publicly available ConvNet
  packages.

- Sliding window conv nets can be implemented using max fragment pooling.
- Increasing the input/output sizes reduces number of operations required by a single pixel.
-

** Claims
*** Use large output patches

    Saturation ~ to the field of view

*** Don't use batch inputs
*** Speed depends on the network depth, width and filter sizes
*** Worth spending time on optimizing the performances as it will save more time overall
*** Instead of buying 3 extra GPUs to get 4x improvement in the throughput, consider buying extra memory (cheaper and possibly better)
*** question: how/why is this different from training
**** throughput rather than latency (often orthogonal goals)
**** no images need to be stored from forward to backward pass
** Intro

   Explain convolutional networks, how the forward pass works
   Explain how it can be done using FFTs

** Algorithms
*** Max fragment pooling

    With restricted output sizes to make sure all the batches have the
    same size

*** CPU
**** Version 1
     - Perform the sequential algorithm.
       - calculate the FFTs of all the inputs
       - for each output
         - for each incoming kernel
           - FFT of kernel
           - point-wise multiply add with the appropriate input FFT
         - end
         - inverse FFT of the output image
       - end
     - Each FFT is parallelized by having multiple thread do a subset
       of 1D convolutions across first x, then y then z - 3 sync
       points.
     - Point-wise operations on the images are also parallelized by
       having multiple threads do different subset of elements [linear
       separation in memmory].
**** Version 2
    - (batch x number of inputs) tasks are generated for computing
      FFTs of the inputs.  Execute in parallel on n workers and wait
      until the last one completes.
    - each of n workers
      - takes kernel from least complete output (removes output from lock-free list)
      - computes kernel FFT
      - creates sub-tasks for each input in a batch for multiply-adds associated with this kernel and this output
      - adds output back to lock-free list
    - (batch x number of outputs) tasks generated for inverse FFTs of
      outputs. Execute in parallel on n workers and wait until the
      last one completes.
*** GPU (onboard RAM)
**** direct convolution
***** cuDNN primitives for 3D convolution (GEMM or precomputed GEMM)
**** FFT-based algorithm
***** loop over batch
****** compute FFTs of all inputs in layer
***** release memory used for inputs
***** loop over outputs in layer
****** compute FFTs of all incident kernels
****** loop over batch
******* point-wise multiplication of all input and kernel FFTs
******* accumulation of output expressed as matrix multiplication
vector multiply,  parallelized using by cuBLAS gemv.
***** release memory used for input FFTs
***** loop over batch
****** inverse FFT of all outputs in layer
***** release memory used for output FFTs
**** Padded Pruned FFTs
***** each batch of 3D FFTs is parallelized
either batch of input images
or batch of all kernels incident on one output image
treat as 4D array
***** for each of last three dimensions
****** transpose array so that
******* dimension is adjacent in memory
******* zero padded for good FFT size
******* parallelized using cuda Thrust
******** memory read/write
******** index arithmetic fast div/mod
****** do 1D FFTs in parallel batches (multiple of 512) using cuFFT
     over batches of 1D FFTs using cuFFT
       (cufftMakePlanMany).
****** note: no need to transpose array to original state as long as
******* all FFTs leave array in same state
******* IFFT does the opposite thing as FFT
*** GPU (onboard and host RAM)
**** algorithm for a layer
***** for each input in batch
****** for each subset of output nodes do
******* for each subset of input nodes
******** load input from host
******** do previous algorithm
******* move result to host
**** analysis shows that
***** subset of output nodes should be as large as possible
***** subset of input nodes might be only one
*** CPU-GPU Fusion

    CPU on the top layers, generating batches (pooling networks).  A
    single batch can then fit on the GPU and is being executed there.

    Pipeline form.

    More limits on the size of the network b/c we need to keep extra
    data in memory (for the pipeline)

** Experiments

   Done on 3D networks with relatively large fields of view

*** Purely convolutional networks
*** Convolutional networks with pooling layers
** Contributions

   Parallel CPU algorithms
   Pruned FFTs on the CPU

   Parallel GPU algorithms using 1D FFTs.
