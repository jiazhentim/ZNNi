
* ZNNi - Maximizing the Inference Throughput of 3D Convolutional Networks on Multi-Core Machines and GPUs.
** Claims
*** Use large output patches

    Saturation ~ to the field of view

*** Don't use batch inputs
*** Speed depends on the network depth, width and filter sizes
*** Worth spending time on optimizing the performances as it will save more time overall
*** Instead of buying 3 extra GPUs to get 4x improvement in the throughput, consider buying extra memory (cheaper and possibly better)
*** question: how/why is this different from training
**** throughput rather than latency (often orthogonal goals)
**** no images need to be stored from forward to backward pass
** Intro

   Explain convolutional networks, how the forward pass works
   Explain how it can be done using FFTs

** Algorithms
*** Max fragment pooling

    With restricted output sizes to make sure all the batches have the
    same size

*** CPU
**** Version 1
     - Perform the sequential algorithm.
       - calculate the FFTs of all the inputs
       - for each output
         - for each incoming kernel
           - FFT of kernel
           - point-wise multiply add with the appropriate input FFT 
         - end
         - inverse FFT of the output image
       - end
     - Each FFT is parallelized by having multiple thread do a subset
       of 1D convolutions across first x, then y then z - 3 sync
       points.
     - Point-wise operations on the images are also parallelized by
       having multiple threads do different subset of elements [linear
       separation in memmory].
**** Version 2
    - (batch x number of inputs) tasks are generated for computing
      FFTs of the inputs.  Execute in parallel on n workers and wait
      until the last one completes.
    - each of n workers
      - takes kernel from least complete output (removes output from lock-free list)
      - computes kernel FFT
      - creates sub-tasks for each input in a batch for multiply-adds associated with this kernel and this output
      - adds output back to lock-free list
    - (batch x number of outputs) tasks generated for inverse FFTs of
      outputs. Execute in parallel on n workers and wait until the
      last one completes.
*** GPU (onboard RAM)
**** direct convolution
***** cuDNN primitives for 3D convolution (GEMM or precomputed GEMM)
**** FFT-based algorithm
***** loop over batch
****** compute FFTs of all inputs in layer
***** release memory used for inputs
***** loop over outputs in layer
****** compute FFTs of all incident kernels
****** loop over batch
******* point-wise multiplication of all input and kernel FFTs
******* accumulation of output expressed as matrix multiplication
vector multiply,  parallelized using by cuBLAS gemv.
***** release memory used for input FFTs
***** loop over batch
****** inverse FFT of all outputs in layer
***** release memory used for output FFTs
**** Padded Pruned FFTs
***** each batch of 3D FFTs is parallelized  
either batch of input images
or batch of all kernels incident on one output image
treat as 4D array
***** for each of last three dimensions
****** transpose array so that 
******* dimension is adjacent in memory
******* zero padded for good FFT size
******* parallelized using cuda Thrust
******** memory read/write
******** index arithmetic fast div/mod
****** do 1D FFTs in parallel batches (multiple of 512) using cuFFT
     over batches of 1D FFTs using cuFFT
       (cufftMakePlanMany).
****** note: no need to transpose array to original state as long as 
******* all FFTs leave array in same state
******* IFFT does the opposite thing as FFT
*** GPU (onboard and host RAM)
**** algorithm for a layer
***** for each input in batch
****** for each subset of output nodes do
******* for each subset of input nodes
******** load input from host
******** do previous algorithm
******* move result to host
**** analysis shows that
***** subset of output nodes should be as large as possible
***** subset of input nodes might be only one
*** CPU-GPU Fusion

    CPU on the top layers, generating batches (pooling networks).  A
    single batch can then fit on the GPU and is being executed there.

    Pipeline form.

    More limits on the size of the network b/c we need to keep extra
    data in memory (for the pipeline)

** Experiments

   Done on 3D networks with relatively large fields of view

*** Purely convolutional networks
*** Convolutional networks with pooling layers
** Contributions

   Parallel CPU algorithms
   Pruned FFTs on the CPU

   Parallel GPU algorithms using 1D FFTs.
