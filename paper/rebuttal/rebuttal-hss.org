In our excitement to pack the paper with results, we evidently failed
to explain novelty clearly, and presented insufficient analysis of
results.  We will revise the paper accordingly.

The conceptual novelty is that we address the acceleration of ConvNet
inference as distinct from training.  For sliding window inference,
processing a large input patch is crucial to achieve high throughput,
and this requires efficient use of RAM.

Our system is the first application of pruned FFTs to any kind of deep
learning.  Other elements are novel in that they are
inference-optimized, and so designed to increase memory locality while
minimizing memory overhead and maximizing utilization of cores.  These
elements include 3D pruned FFTs, primitives for multicore CPU, GPU,
and GPU+host RAM, and a pipelined implementation utilizing both CPU
and GPU.

In contrast, Caffe con Troll (CcT), mentioned by R2, employs only a
simple kind of data parallelism, processing multiple input patches in
parallel.  Such "batch parallelism" is reasonable for training, but
suboptimal for inference because to fit in RAM the input patches must
be small when the batch size is large. On our 72-core machine,
CcT-style data parallelism would limit the memory per input patch to
256/72=3.5 GB, which would reduce throughput by 10x or more (Fig4).
(Note that CcT cannot be directly applied to our benchmark tasks
because it handles neither 3D nor sliding window inference.)

For CPU+GPU, CcT-style data parallelism has large communication
overhead due to multiple transfers to and from the GPU for all the
layers of the network.  Our novel approach instead involves a single
"handoff" from CPU to GPU at an intermediate layer of the network, and
is pipelined to keep both CPU and GPU working all the time.  (R2's
remark that CcT is pipelined is incorrect.)

Our analyses of results may have been difficult to understand because
they were scattered throughout the text.  We will consolidate the
analyses in a single section including details on:

   - How are the measurements performed (time includes all the
     transfers to and from the device, and is averaged over 20
     iterations).
   - Additional analysis of the measurements (e.g. why are the GPU
     lines dropping)
   - Provide intuition on why our approach greatly outperforms the
     competition.

We will improve the conclusion by adding

   - A guideline for choosing optimal hardware when optimizing for
     inference.
   - List applications that can benefit from out approach.


Answers to specific questions:

R1:

- cuDNNv4 was indeed used for direct convolution.  Table IV might be
  the source of confusion, as cuDNN1 and cuDNN2 referred to two
  different versions of GPU primitives in cuDNNv4 rather than old
  cuDNN versions.
- We could have implemented our own primitives instead of using cuDNN.
  However, we estimate that cuDNNv4 attains more than 60% of the
  theoretical maximum FLOPS, leaving little room for improvement.

R2:

- As far as we know, this is the first application of pruned FFTs to
  deep learning.
- Pruning the FFT of a 3D kernel reduces the FLOPs by 3x. Additional
  speedup (to 5x) is due to the fact that the pruned FFTs are the ones
  with low memory locality.
- The paper compares our problem-specific task scheduling explicitly
  and quantitatively relative to the standard alternative of "work
  stealing."
- Because buffer sizes vary, we allocate and de-allocate memory to use
  it as efficiently as possible.  Reuse of a preallocated buffer would
  be less efficient.

R3:

- The benchmarked network architectures are representative of the
  state of the art in dense prediction tasks like image
  segmentation. The architectures use many convolution and pooling
  layers to achieve a large field of view.  The architectures with
  smaller kernels are more typical, but we have also included some
  with larger kernels.

R4:

- "Kernel" refers to the convolution kernel, also known as the
  filter. It should not be confused with the GPU kernel.
- For any kind of FFT convolution, the image and kernel have to be
  padded to a common size.  We choose this size to be a product of
  small primes, which speeds up the FFT as is well-known.
- Our data parallel and task parallel primitives are optimized for
  inference, and are not at all standard because previous efforts
  optimized for training. Our primitives achieve 10x better throughput
  relative to other published approaches.
- Max pooling is a standard type of nonlinearity used in ConvNets.
- The GPU+host RAM convolution addresses the bottleneck induced by the
  limited onboard RAM of the GPU.
